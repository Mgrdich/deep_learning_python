{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Footnotes\n",
    "let's review the classic evaluation recipes\n",
    "* Simple hold-out validation\n",
    "* K-fold validation\n",
    "* iterated K-fold validation with shuffling\n",
    "\n",
    "### Simple Hold-out validation\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and\n",
    "evaluate on the test set. As you saw in the previous sections, in order to prevent\n",
    "information leaks, you shouldn’t tune your model based on the test set, and therefore you\n",
    "should also reserve a validation set.\n",
    "\n",
    "#### Cons\n",
    "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is\n",
    "available, then your validation and test sets may contain too few samples to be statisti-\n",
    "cally representative of the data at hand.\n",
    "\n",
    "\n",
    "### K-Fold Validation\n",
    "K- FOLD VALIDATION\n",
    "With this approach, you split your data into K partitions of equal size. For each parti-\n",
    "tion i , train a model on the remaining K – 1 partitions, and evaluate it on partition i .\n",
    "Your final score is then the averages of the K scores obtained.\n",
    "\n",
    "### Iterated K-fold With Shuffling\n",
    "This one is for situations in which you have relatively little data available and you need\n",
    "to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in\n",
    "Kaggle competitions. It consists of applying K -fold validation multiple times, shuffling\n",
    "the data every time before splitting it K ways. The final score is the average of the\n",
    "scores obtained at each run of K -fold validation. Note that you end up training and\n",
    "evaluating `P × K` models (where `P` is the number of iterations you use), which can very\n",
    "expensive.\n",
    "\n",
    "\n",
    "## Data preprocessing feature engineering and feature learning\n",
    "* VECTORIZATION\n",
    "* VALUE NORMALIZATION\n",
    "\n",
    "### Vectorization\n",
    "Whatever data you need to process—sound,\n",
    "images, text—you must first turn into tensors, a step called data vectorization. For\n",
    "instance, in the two previous text-classification examples, we started from text repre-\n",
    "sented as lists of integers (standing for sequences of words)\n",
    "\n",
    "### Value Normalization\n",
    "Before you fed this data into your network,\n",
    "you had to normalize each feature independently so that it had a standard deviation\n",
    "of 1 and a mean of 0.\n",
    "\n",
    "In general, it isn’t safe to feed into a neural network data that takes relatively large val-\n",
    "ues (for example, multidigit integers, which are much larger than the initial values taken\n",
    "by the weights of a network) or data that is heterogeneous\n",
    "\n",
    "\n",
    "### What Should be done\n",
    "* Take small values - btw range 0 - 1\n",
    "* Be homogenous - That is all features should take values in roughly the same range\n",
    "\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "The fundamental issue in machine learning is the tension between optimization\n",
    "and generalization\n",
    "\n",
    "**Optimization** to the process of adjusting a model to get the\n",
    "best performance possible on the training data if still underfit state\n",
    "\n",
    "The processing of fighting overfitting this way is called **regularization**. Let’s review\n",
    "some of the most common regularization techniques\n",
    "\n",
    "### Reduce the network's size\n",
    "The simplest way to prevent overfitting is to reduce the size of the model: the number\n",
    "of learnable parameters in the model (which is determined by the number of layers\n",
    "and the number of units per layer)\n",
    "\n",
    "On the other hand, if the network has limited memorization resources, it won’t be\n",
    "able to learn this mapping as easily; thus, in order to minimize its loss, it will have to\n",
    "resort to learning compressed representations that have predictive power regarding\n",
    "the targets—precisely the type of representations we’re interested in. At the same\n",
    "time, keep in mind that you should use models that have enough parameters that they\n",
    "don’t underfit:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}